{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee518ed-c61d-4aa4-ba98-1b5b090b0f40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tj\\.conda\\envs\\py3.7\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%config Completer.use_jedi = False\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3bd18-8959-4bd0-940a-6feedf51e886",
   "metadata": {},
   "source": [
    "***\n",
    "지문읽고 주제 분류하기\n",
    "***\n",
    "\n",
    "다음과 같은 데이터를 데이터프레임으로 저장한다.  \n",
    "입력 데이터는 음식 및 스포츠 관련 지문으로 구성되어 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66d9b7a-2d30-47f5-bb14-ab55ed21010d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_dict_list = [\n",
    "\t{'paragraph': 'Dishplace is located in sunnyvale downtown there is parking around the area but it can be difficult to find during peak business hours my sisters and i came to this place for dinner on a weekday they were really busy so i highly recommended making reservations unless you have the patience to wait', 'category': 'food'},\n",
    "\t{'paragraph': 'Service can be slower during busy hours but our waiter was courteous and help gave some great entree recommendations', 'category': 'food'},\n",
    "\t{'paragraph': 'Portions are huge both french toast and their various omelettes are really good their french toast is probably 1.5x more than other brunch places great place to visit if you are hungry and dont want to wait 1 hour for a table', 'category': 'food'},\n",
    "\t{'paragraph': 'We started with apps going the chicken and waffle slides and chicken nachos the sliders were amazing and the nachos were good too maybe by themselves the nachos would have scored better but after those sliders they were up against some tough competition', 'category': 'food'},\n",
    "\t{'paragraph': 'The biscuits and gravy was too salty two people in my group had the gravy and all thought it was too salty my hubby ordered a side of double egg and it was served on two small plates who serves eggs to one person on separate plates we commented on that when it was delivered and even the server laughed and said she doesnt know why the kitchen does that presentation of food is important and they really missed on this one', 'category': 'food'},\n",
    "\t{'paragraph': 'The garlic fries were a great starter (and a happy hour special) the pancakes looked and tasted great and were a fairly generous portion', 'category': 'food'},\n",
    "\t{'paragraph': 'Our meal was excellent i had the pasta ai formaggi which was so rich i didnt dare eat it all although i certainly wanted to excellent flavors with a great texture contrast between the soft pasta and the crisp bread crumbs too much sauce for me but a wonderful dish', 'category': 'food'},\n",
    "\t{'paragraph': 'What i enjoy most about palo alto is so many restaurants have dog-friendly seating outside i had bookmarked italico from when they first opened about a 1.5 years ago and was jonesing for some pasta so time to finally knock that bookmark off', 'category': 'food'},\n",
    "\t{'paragraph': 'The drinks came out fairly quickly a good two to three minutes after the orders were taken i expected my iced tea to taste a bit more sweet but this was straight up green tea with ice in it not to complain of course but i was pleasantly surprised', 'category': 'food'},\n",
    "\t{'paragraph': 'Despite the not so good burger the service was so slow the restaurant wasnt even half full and they took very long from the moment we got seated to the time we left it was almost 2 hours we thought that it would be quick since we ordered as soon as we sat down my coworkers did seem to enjoy their beef burgers for those who eat beef however i will not be returning it is too expensive and extremely slow service', 'category': 'food'},\n",
    "\t{'paragraph': 'The four reigning major champions simona halep caroline wozniacki angelique kerber and defending us open champion sloane stephens could make a case for being the quartet most likely to succeed especially as all but stephens has also enjoyed the no1 ranking within the last 14 months as they prepare for their gruelling new york campaigns they currently hold the top four places in the ranks', 'category': 'sports'},\n",
    "\t{'paragraph': 'The briton was seeded nn7 here last year before a slump in form and confidence took her down to no46 after five first-round losses but there have been signs of a turnaround including a victory over a sub-par serena williams in san jose plus wins against jelena ostapenko and victoria azarenka in montreal. konta pulled out of new haven this week with illness but will hope for good things where she first scored wins in a major before her big breakthroughs to the semis in australia and wimbledon', 'category': 'sports'},\n",
    "\t{'paragraph': 'Stephens surged her way back from injury in stunning style to win her first major here last year—and ranked just no83 she has since proved what a big time player she is winning the miami title via four fellow major champions then reaching the final at the french open back on north american hard courts she ran to the final in montreal only just edged out by halep she has also avoided many of the big names in her quarter—except for wild card azarenka as a possible in the third round', 'category': 'sports'},\n",
    "\t{'paragraph': 'When it came to england chances in the world cup it would be fair to say that most fans had never been more pessimistic than they were this year after enduring years of truly dismal performances at major tournaments – culminating in the 2014 event where they failed to win any of their three group games and finished in bottom spot those results led to the resignation of manager roy hodgson', 'category': 'sports'},\n",
    "\t{'paragraph': 'The team that eliminated russia – croatia – also improved enormously during the tournament before it began their odds were 33/1 but they played with real flair and star players like luka modric ivan rakitic and ivan perisic showed their quality on the world stage having displayed their potential by winning all three of their group stage games croatia went on to face difficult tests like the semi-final against england', 'category': 'sports'},\n",
    "\t{'paragraph': 'The perseyside outfit finished in fourth place in the premier league table and without a trophy last term after having reached the champions league final before losing to real madrid', 'category': 'sports'},\n",
    "\t{'paragraph': 'Liverpool fc will return to premier league action on saturday lunchtime when they travel to leicester city in the top flight as they look to make it four wins in a row in the league', 'category': 'sports'},\n",
    "\t{'paragraph': 'Alisson signed for liverpool fc from as roma this summer and the brazilian goalkeeper has helped the reds to keep three clean sheets in their first three premier league games', 'category': 'sports'},\n",
    "\t{'paragraph': 'But the rankings during that run-in to new york hid some very different undercurrents for murray had struggled with a hip injury since the clay swing and had not played a match since losing his quarter-final at wimbledon and he would pull out of the us open just two days before the tournament began—too late however to promote nederer to the no2 seeding', 'category': 'sports'},\n",
    "\t{'paragraph': 'Then came the oh-so-familiar djokovic-nadal no-quarter-given battle for dominance in the third set there were exhilarating rallies with both chasing to the net both retrieving what looked like winning shots nadal more than once pulled off a reverse smash and had his chance to seal the tie-break but it was djokovic serving at 10-9 who dragged one decisive error from nadal for a two-sets lead', 'category': 'sports'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea89f98b-4513-4532-a974-af826788814a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph category\n",
       "0  Dishplace is located in sunnyvale downtown the...     food\n",
       "1  Service can be slower during busy hours but ou...     food\n",
       "2  Portions are huge both french toast and their ...     food\n",
       "3  We started with apps going the chicken and waf...     food\n",
       "4  The biscuits and gravy was too salty two peopl...     food"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(paragraph_dict_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5c1e4-1610-468f-a5f4-5464ae5434e8",
   "metadata": {},
   "source": [
    "***\n",
    "데이터전처리\n",
    "***\n",
    "LSTM 모델이 입력 데이터를 처리할 수 있도록 수치값으로 변경한다.  \n",
    "텍스트인 입력값을 수치로 변환하기 위해서 지문에 사용된 모든 단어들을 모아서 중복을 제거한 후 단어 리스트를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c49659-9ba0-4714-9284-28d579ee2ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n",
      "{'the', 'important', 'busy', 'had', 'modric', 'ice', 'croatia', 'murray', 'form', 'patience', 'delivered', 'french', 'seeded', 'recommended', 'palo', 'began', 'sloane', 'perseyside', 'tea', 'brazilian', 'down', 'england', 'places', 'confidence', 'expensive', 'return', 'five', 'they', 'year', 'dismal', 'halep', 'themselves', 'their', '14', 'spot', 'soon', 'probably', 'two', '2', 'bottom', 'tests', 'certainly', 'business', 'reservations', 'australia', 'chances', 'chicken', 'up', 'making', 'where', 'courts', 'courteous', 'didnt', 'djokovic-nadal', 'brunch', 'years', 'separate', 'alto', 'wanted', 'dog-friendly', 'around', 'what', 'hold', 'breakthroughs', 'dragged', 'crisp', 'victoria', 'after', 'flair', 'wozniacki', 'win', 'rich', 'including', 'burger', 'meal', 'hope', 'smash', 'i', 'too', 'fellow', 'happy', 'got', 'much', 'portions', 'performances', 'angelique', 'food', 'retrieving', 'reverse', '–', 'tournament', 'waiter', 'soft', 'konta', 'slow', 'things', 'she', 'dishplace', 'sat', 'gruelling', 'injury', 'ordered', 'lunchtime', 'our', 'serves', 'competition', 'bookmarked', 'luka', 'premier', 'pull', 'no1', 'truly', 'a', 'wasnt', 'champion', 'displayed', 'enduring', 'would', 'pleasantly', 'commented', 'wonderful', 'roy', 'presentation', 'fourth', 'texture', 'between', 'amazing', 'visit', 'jose', 'row', 'once', 'bookmark', 'edged', 'my', 'be', 'ranking', 'difficult', 'find', 'downtown', 'league', 'began—too', 'recommendations', 'italico', 'reds', 'for', 'is', 'apps', 'all', '(and', 'ago', 'tough', 'me', 'came', 'undercurrents', 'parking', 'out', 'match', 'set', 'leicester', 'enormously', 'started', 'during', 'those', 'as', 'against', 'back', 'final', 'table', 'madrid', 'on', 'slump', 'pulled', 'ai', '2014', 'but', 'so', 'briton', 'term', 'djokovic', 'time', 'led', 'only', 'entree', 'most', 'two-sets', 'manager', 'serving', 'improved', 'want', 'about', 'montreal.', 'laughed', 'green', 'contrast', 'way', 'dominance', 'clay', 'seeding', 'currently', 'without', 'server', 'swing', 'dare', 'stage', 'hard', 'cup', 'peak', 'service', 'this', 'starter', 'nachos', 'clean', 'style', 'new', 'make', 'nederer', 'real', 'campaigns', 'located', 'help', 'having', 'almost', 'rankings', 'know', 'special)', 'nadal', 'late', 'outside', 'group', 'garlic', 'generous', 'been', 'jelena', 'ivan', 'some', 'other', 'different', 'enjoyed', 'complain', 'scored', 'travel', 'jonesing', 'weekday', 'opened', 'any', 'surged', 'taken', 'decisive', 'players', 'shots', 'title', 'it', 'possible', 'slides', 'dont', 'returning', 'goalkeeper', 'took', 'eliminated', 'surprised', 'very', 'results', 'enjoy', 'even', 'san', 'no2', 'wild', 'sisters', 'no-quarter-given', 'major', 'toast', 'then', 'at', 'huge', 'went', 'keep', 'helped', 'restaurants', 'just', 'days', 'beef', 'sub-par', 'kerber', 'full', 'he', 'knock', 'missed', 'was', 'when', 'more', 'simona', 'left', 'player', 'fans', 'names', 'quality', 'third', 'bit', 'exhilarating', 'week', 'coworkers', 'succeed', 'odds', 'caroline', 'look', 'tasted', 'eggs', 'hid', 'big', 'who', '1.5', 'extremely', 'being', 'no46', 'minutes', 'by', 'quarter-final', '33/1', 'culminating', 'losses', 'wait', 'served', 'highly', 'seal', 'reached', 'azarenka', 'despite', 'months', 'case', 'plus', 'going', 'to', 'city', 'resignation', 'wins', 'lead', 'although', 'said', 'say', 'bread', 'half', 'drinks', 'liverpool', 'unless', 'potential', 'haven', 'north', 'not', 'seating', 'proved', 'outfit', 'error', 'defending', 'seem', 'last', 'dinner', 'finally', 'quick', 'ranks', 'trophy', 'four', 'semis', 'alisson', 'has', 'quickly', 'can', 'both', 'tournaments', 'dish', 'pancakes', 'did', 'stunning', 'thought', 'signed', 'in', 'played', 'flight', 'summer', 'action', 'fair', 'nn7', 'world', 'three', 'champions', 'area', 'expected', 'eat', 'miami', 'which', 'ranked', 'biscuits', 'that', 'ran', 'especially', 'before', 'first', 'open', 'one', 'never', 'top', 'event', 'promote', 'battle', 'crumbs', 'from', 'reaching', 'tie-break', 'straight', 'russia', 'fc', 'star', 'rallies', 'taste', 'net', 'quartet', 'side', 'struggled', 'round', 'games', 'serena', 'burgers', 'double', 'off', 'course', 'rakitic', 'likely', 'face', 'his', 'better', 'does', 'formaggi', 'various', 'reigning', 'illness', 'fairly', 'really', 'egg', 'kitchen', 'hodgson', '1.5x', 'over', 'wimbledon', 'moment', 'many', 'you', 'chasing', 'sauce', 'iced', 'hour', 'within', 'flavors', 'omelettes', 'sweet', 'we', '1', 'winning', 'of', 'waffle', 'saturday', 'also', 'stephens', 'american', 'doesnt', 'portion', 'plates', 'excellent', 'with', 'team', 'than', 'seated', 'hip', 'looked', 'turnaround', 'montreal', 'and', 'were', 'us', 'first-round', 'pessimistic', 'fries', 'quarter—except', 'have', 'however', 'could', 'no83', 'signs', 'person', 'finished', 'via', 'here', 'orders', 'williams', 'showed', 'long', 'there', 'roma', 'victory', 'gave', 'perisic', '10-9', 'gravy', 'good', 'salty', 'small', 'ostapenko', 'restaurant', 'sheets', 'sliders', 'year—and', 'losing', 'prepare', 'place', 'her', 'great', 'why', 'sunnyvale', 'card', 'maybe', 'chance', 'run-in', 'avoided', 'pasta', 'slower', 'if', 'york', 'hubby', 'semi-final', 'oh-so-familiar', 'hours', 'people', 'hungry', 'like', 'are', 'failed', 'since', 'will'}\n"
     ]
    }
   ],
   "source": [
    "# 중복을 해서 지문에 사용된 단어를 모으기 위해 빈 set을 선언한다.\n",
    "results = set()\n",
    "\n",
    "# 데이터프레임의 paragraph열(Series)을 모두 소문자로 변환한 후 공백을 경계로 나눠서 set에 저장한다.\n",
    "#print(type(results)) # class 'set'\n",
    "#print(type(df)) # dataframe\n",
    "#print(type(df.paragraph)) # Series\n",
    "#print(type(df.paragraph.str)) #StringMethod\n",
    "#print(type(df.paragraph.str.lower())) #Series\n",
    "#print(type(df.paragraph.str.lower().str))# StringMethod\n",
    "#print(type(df.paragraph.str.lower().str.split(' '))) # Series\n",
    "df.paragraph.str.lower().str.split(' ').apply(results.update)\n",
    "print(len(results))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa3a8e7-fe38-4cdb-bd80-487cd9f431e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "set에 저장된 데이터를 아래와 같은 형태의 딕셔너리로 만든다.\n",
    "***\n",
    "{ 0 : 'year', 1 : 'tested', 2 : 'gravy', .... , 535 : 'left'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0af74f6-d80b-4ba5-be87-30f13598f602",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{0: 'the', 1: 'important', 2: 'busy', 3: 'had', 4: 'modric', 5: 'ice', 6: 'croatia', 7: 'murray', 8: 'form', 9: 'patience', 10: 'delivered', 11: 'french', 12: 'seeded', 13: 'recommended', 14: 'palo', 15: 'began', 16: 'sloane', 17: 'perseyside', 18: 'tea', 19: 'brazilian', 20: 'down', 21: 'england', 22: 'places', 23: 'confidence', 24: 'expensive', 25: 'return', 26: 'five', 27: 'they', 28: 'year', 29: 'dismal', 30: 'halep', 31: 'themselves', 32: 'their', 33: '14', 34: 'spot', 35: 'soon', 36: 'probably', 37: 'two', 38: '2', 39: 'bottom', 40: 'tests', 41: 'certainly', 42: 'business', 43: 'reservations', 44: 'australia', 45: 'chances', 46: 'chicken', 47: 'up', 48: 'making', 49: 'where', 50: 'courts', 51: 'courteous', 52: 'didnt', 53: 'djokovic-nadal', 54: 'brunch', 55: 'years', 56: 'separate', 57: 'alto', 58: 'wanted', 59: 'dog-friendly', 60: 'around', 61: 'what', 62: 'hold', 63: 'breakthroughs', 64: 'dragged', 65: 'crisp', 66: 'victoria', 67: 'after', 68: 'flair', 69: 'wozniacki', 70: 'win', 71: 'rich', 72: 'including', 73: 'burger', 74: 'meal', 75: 'hope', 76: 'smash', 77: 'i', 78: 'too', 79: 'fellow', 80: 'happy', 81: 'got', 82: 'much', 83: 'portions', 84: 'performances', 85: 'angelique', 86: 'food', 87: 'retrieving', 88: 'reverse', 89: '–', 90: 'tournament', 91: 'waiter', 92: 'soft', 93: 'konta', 94: 'slow', 95: 'things', 96: 'she', 97: 'dishplace', 98: 'sat', 99: 'gruelling', 100: 'injury', 101: 'ordered', 102: 'lunchtime', 103: 'our', 104: 'serves', 105: 'competition', 106: 'bookmarked', 107: 'luka', 108: 'premier', 109: 'pull', 110: 'no1', 111: 'truly', 112: 'a', 113: 'wasnt', 114: 'champion', 115: 'displayed', 116: 'enduring', 117: 'would', 118: 'pleasantly', 119: 'commented', 120: 'wonderful', 121: 'roy', 122: 'presentation', 123: 'fourth', 124: 'texture', 125: 'between', 126: 'amazing', 127: 'visit', 128: 'jose', 129: 'row', 130: 'once', 131: 'bookmark', 132: 'edged', 133: 'my', 134: 'be', 135: 'ranking', 136: 'difficult', 137: 'find', 138: 'downtown', 139: 'league', 140: 'began—too', 141: 'recommendations', 142: 'italico', 143: 'reds', 144: 'for', 145: 'is', 146: 'apps', 147: 'all', 148: '(and', 149: 'ago', 150: 'tough', 151: 'me', 152: 'came', 153: 'undercurrents', 154: 'parking', 155: 'out', 156: 'match', 157: 'set', 158: 'leicester', 159: 'enormously', 160: 'started', 161: 'during', 162: 'those', 163: 'as', 164: 'against', 165: 'back', 166: 'final', 167: 'table', 168: 'madrid', 169: 'on', 170: 'slump', 171: 'pulled', 172: 'ai', 173: '2014', 174: 'but', 175: 'so', 176: 'briton', 177: 'term', 178: 'djokovic', 179: 'time', 180: 'led', 181: 'only', 182: 'entree', 183: 'most', 184: 'two-sets', 185: 'manager', 186: 'serving', 187: 'improved', 188: 'want', 189: 'about', 190: 'montreal.', 191: 'laughed', 192: 'green', 193: 'contrast', 194: 'way', 195: 'dominance', 196: 'clay', 197: 'seeding', 198: 'currently', 199: 'without', 200: 'server', 201: 'swing', 202: 'dare', 203: 'stage', 204: 'hard', 205: 'cup', 206: 'peak', 207: 'service', 208: 'this', 209: 'starter', 210: 'nachos', 211: 'clean', 212: 'style', 213: 'new', 214: 'make', 215: 'nederer', 216: 'real', 217: 'campaigns', 218: 'located', 219: 'help', 220: 'having', 221: 'almost', 222: 'rankings', 223: 'know', 224: 'special)', 225: 'nadal', 226: 'late', 227: 'outside', 228: 'group', 229: 'garlic', 230: 'generous', 231: 'been', 232: 'jelena', 233: 'ivan', 234: 'some', 235: 'other', 236: 'different', 237: 'enjoyed', 238: 'complain', 239: 'scored', 240: 'travel', 241: 'jonesing', 242: 'weekday', 243: 'opened', 244: 'any', 245: 'surged', 246: 'taken', 247: 'decisive', 248: 'players', 249: 'shots', 250: 'title', 251: 'it', 252: 'possible', 253: 'slides', 254: 'dont', 255: 'returning', 256: 'goalkeeper', 257: 'took', 258: 'eliminated', 259: 'surprised', 260: 'very', 261: 'results', 262: 'enjoy', 263: 'even', 264: 'san', 265: 'no2', 266: 'wild', 267: 'sisters', 268: 'no-quarter-given', 269: 'major', 270: 'toast', 271: 'then', 272: 'at', 273: 'huge', 274: 'went', 275: 'keep', 276: 'helped', 277: 'restaurants', 278: 'just', 279: 'days', 280: 'beef', 281: 'sub-par', 282: 'kerber', 283: 'full', 284: 'he', 285: 'knock', 286: 'missed', 287: 'was', 288: 'when', 289: 'more', 290: 'simona', 291: 'left', 292: 'player', 293: 'fans', 294: 'names', 295: 'quality', 296: 'third', 297: 'bit', 298: 'exhilarating', 299: 'week', 300: 'coworkers', 301: 'succeed', 302: 'odds', 303: 'caroline', 304: 'look', 305: 'tasted', 306: 'eggs', 307: 'hid', 308: 'big', 309: 'who', 310: '1.5', 311: 'extremely', 312: 'being', 313: 'no46', 314: 'minutes', 315: 'by', 316: 'quarter-final', 317: '33/1', 318: 'culminating', 319: 'losses', 320: 'wait', 321: 'served', 322: 'highly', 323: 'seal', 324: 'reached', 325: 'azarenka', 326: 'despite', 327: 'months', 328: 'case', 329: 'plus', 330: 'going', 331: 'to', 332: 'city', 333: 'resignation', 334: 'wins', 335: 'lead', 336: 'although', 337: 'said', 338: 'say', 339: 'bread', 340: 'half', 341: 'drinks', 342: 'liverpool', 343: 'unless', 344: 'potential', 345: 'haven', 346: 'north', 347: 'not', 348: 'seating', 349: 'proved', 350: 'outfit', 351: 'error', 352: 'defending', 353: 'seem', 354: 'last', 355: 'dinner', 356: 'finally', 357: 'quick', 358: 'ranks', 359: 'trophy', 360: 'four', 361: 'semis', 362: 'alisson', 363: 'has', 364: 'quickly', 365: 'can', 366: 'both', 367: 'tournaments', 368: 'dish', 369: 'pancakes', 370: 'did', 371: 'stunning', 372: 'thought', 373: 'signed', 374: 'in', 375: 'played', 376: 'flight', 377: 'summer', 378: 'action', 379: 'fair', 380: 'nn7', 381: 'world', 382: 'three', 383: 'champions', 384: 'area', 385: 'expected', 386: 'eat', 387: 'miami', 388: 'which', 389: 'ranked', 390: 'biscuits', 391: 'that', 392: 'ran', 393: 'especially', 394: 'before', 395: 'first', 396: 'open', 397: 'one', 398: 'never', 399: 'top', 400: 'event', 401: 'promote', 402: 'battle', 403: 'crumbs', 404: 'from', 405: 'reaching', 406: 'tie-break', 407: 'straight', 408: 'russia', 409: 'fc', 410: 'star', 411: 'rallies', 412: 'taste', 413: 'net', 414: 'quartet', 415: 'side', 416: 'struggled', 417: 'round', 418: 'games', 419: 'serena', 420: 'burgers', 421: 'double', 422: 'off', 423: 'course', 424: 'rakitic', 425: 'likely', 426: 'face', 427: 'his', 428: 'better', 429: 'does', 430: 'formaggi', 431: 'various', 432: 'reigning', 433: 'illness', 434: 'fairly', 435: 'really', 436: 'egg', 437: 'kitchen', 438: 'hodgson', 439: '1.5x', 440: 'over', 441: 'wimbledon', 442: 'moment', 443: 'many', 444: 'you', 445: 'chasing', 446: 'sauce', 447: 'iced', 448: 'hour', 449: 'within', 450: 'flavors', 451: 'omelettes', 452: 'sweet', 453: 'we', 454: '1', 455: 'winning', 456: 'of', 457: 'waffle', 458: 'saturday', 459: 'also', 460: 'stephens', 461: 'american', 462: 'doesnt', 463: 'portion', 464: 'plates', 465: 'excellent', 466: 'with', 467: 'team', 468: 'than', 469: 'seated', 470: 'hip', 471: 'looked', 472: 'turnaround', 473: 'montreal', 474: 'and', 475: 'were', 476: 'us', 477: 'first-round', 478: 'pessimistic', 479: 'fries', 480: 'quarter—except', 481: 'have', 482: 'however', 483: 'could', 484: 'no83', 485: 'signs', 486: 'person', 487: 'finished', 488: 'via', 489: 'here', 490: 'orders', 491: 'williams', 492: 'showed', 493: 'long', 494: 'there', 495: 'roma', 496: 'victory', 497: 'gave', 498: 'perisic', 499: '10-9', 500: 'gravy', 501: 'good', 502: 'salty', 503: 'small', 504: 'ostapenko', 505: 'restaurant', 506: 'sheets', 507: 'sliders', 508: 'year—and', 509: 'losing', 510: 'prepare', 511: 'place', 512: 'her', 513: 'great', 514: 'why', 515: 'sunnyvale', 516: 'card', 517: 'maybe', 518: 'chance', 519: 'run-in', 520: 'avoided', 521: 'pasta', 522: 'slower', 523: 'if', 524: 'york', 525: 'hubby', 526: 'semi-final', 527: 'oh-so-familiar', 528: 'hours', 529: 'people', 530: 'hungry', 531: 'like', 532: 'are', 533: 'failed', 534: 'since', 535: 'will'}\n"
     ]
    }
   ],
   "source": [
    "# enumerate() 함수는 인수로 지정된 객체에 저장된 내용을 (인덱스, 데이터) 형태로 리턴한다. \n",
    "#for index, word in enumerate(results):\n",
    "     #print(index, word)\n",
    "idx2word = dict(enumerate(results))\n",
    "print(type(idx2word))\n",
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e235c-79a8-4a4d-9dd7-64d48da01e0f",
   "metadata": {},
   "source": [
    "***\n",
    "{ 0 : 'year', 1 : 'tested', 2 : 'gravy', .... , 535 : 'left'} 형태의 딕셔너리를  \n",
    "{ 'year' ; 0, 'tested' : 1,  'gravy' : 2, .... , 'left' : 535 } 형태의 딕셔너리로 변환한다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be7a119-fef3-493e-af15-020e9ff978c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'important': 1, 'busy': 2, 'had': 3, 'modric': 4, 'ice': 5, 'croatia': 6, 'murray': 7, 'form': 8, 'patience': 9, 'delivered': 10, 'french': 11, 'seeded': 12, 'recommended': 13, 'palo': 14, 'began': 15, 'sloane': 16, 'perseyside': 17, 'tea': 18, 'brazilian': 19, 'down': 20, 'england': 21, 'places': 22, 'confidence': 23, 'expensive': 24, 'return': 25, 'five': 26, 'they': 27, 'year': 28, 'dismal': 29, 'halep': 30, 'themselves': 31, 'their': 32, '14': 33, 'spot': 34, 'soon': 35, 'probably': 36, 'two': 37, '2': 38, 'bottom': 39, 'tests': 40, 'certainly': 41, 'business': 42, 'reservations': 43, 'australia': 44, 'chances': 45, 'chicken': 46, 'up': 47, 'making': 48, 'where': 49, 'courts': 50, 'courteous': 51, 'didnt': 52, 'djokovic-nadal': 53, 'brunch': 54, 'years': 55, 'separate': 56, 'alto': 57, 'wanted': 58, 'dog-friendly': 59, 'around': 60, 'what': 61, 'hold': 62, 'breakthroughs': 63, 'dragged': 64, 'crisp': 65, 'victoria': 66, 'after': 67, 'flair': 68, 'wozniacki': 69, 'win': 70, 'rich': 71, 'including': 72, 'burger': 73, 'meal': 74, 'hope': 75, 'smash': 76, 'i': 77, 'too': 78, 'fellow': 79, 'happy': 80, 'got': 81, 'much': 82, 'portions': 83, 'performances': 84, 'angelique': 85, 'food': 86, 'retrieving': 87, 'reverse': 88, '–': 89, 'tournament': 90, 'waiter': 91, 'soft': 92, 'konta': 93, 'slow': 94, 'things': 95, 'she': 96, 'dishplace': 97, 'sat': 98, 'gruelling': 99, 'injury': 100, 'ordered': 101, 'lunchtime': 102, 'our': 103, 'serves': 104, 'competition': 105, 'bookmarked': 106, 'luka': 107, 'premier': 108, 'pull': 109, 'no1': 110, 'truly': 111, 'a': 112, 'wasnt': 113, 'champion': 114, 'displayed': 115, 'enduring': 116, 'would': 117, 'pleasantly': 118, 'commented': 119, 'wonderful': 120, 'roy': 121, 'presentation': 122, 'fourth': 123, 'texture': 124, 'between': 125, 'amazing': 126, 'visit': 127, 'jose': 128, 'row': 129, 'once': 130, 'bookmark': 131, 'edged': 132, 'my': 133, 'be': 134, 'ranking': 135, 'difficult': 136, 'find': 137, 'downtown': 138, 'league': 139, 'began—too': 140, 'recommendations': 141, 'italico': 142, 'reds': 143, 'for': 144, 'is': 145, 'apps': 146, 'all': 147, '(and': 148, 'ago': 149, 'tough': 150, 'me': 151, 'came': 152, 'undercurrents': 153, 'parking': 154, 'out': 155, 'match': 156, 'set': 157, 'leicester': 158, 'enormously': 159, 'started': 160, 'during': 161, 'those': 162, 'as': 163, 'against': 164, 'back': 165, 'final': 166, 'table': 167, 'madrid': 168, 'on': 169, 'slump': 170, 'pulled': 171, 'ai': 172, '2014': 173, 'but': 174, 'so': 175, 'briton': 176, 'term': 177, 'djokovic': 178, 'time': 179, 'led': 180, 'only': 181, 'entree': 182, 'most': 183, 'two-sets': 184, 'manager': 185, 'serving': 186, 'improved': 187, 'want': 188, 'about': 189, 'montreal.': 190, 'laughed': 191, 'green': 192, 'contrast': 193, 'way': 194, 'dominance': 195, 'clay': 196, 'seeding': 197, 'currently': 198, 'without': 199, 'server': 200, 'swing': 201, 'dare': 202, 'stage': 203, 'hard': 204, 'cup': 205, 'peak': 206, 'service': 207, 'this': 208, 'starter': 209, 'nachos': 210, 'clean': 211, 'style': 212, 'new': 213, 'make': 214, 'nederer': 215, 'real': 216, 'campaigns': 217, 'located': 218, 'help': 219, 'having': 220, 'almost': 221, 'rankings': 222, 'know': 223, 'special)': 224, 'nadal': 225, 'late': 226, 'outside': 227, 'group': 228, 'garlic': 229, 'generous': 230, 'been': 231, 'jelena': 232, 'ivan': 233, 'some': 234, 'other': 235, 'different': 236, 'enjoyed': 237, 'complain': 238, 'scored': 239, 'travel': 240, 'jonesing': 241, 'weekday': 242, 'opened': 243, 'any': 244, 'surged': 245, 'taken': 246, 'decisive': 247, 'players': 248, 'shots': 249, 'title': 250, 'it': 251, 'possible': 252, 'slides': 253, 'dont': 254, 'returning': 255, 'goalkeeper': 256, 'took': 257, 'eliminated': 258, 'surprised': 259, 'very': 260, 'results': 261, 'enjoy': 262, 'even': 263, 'san': 264, 'no2': 265, 'wild': 266, 'sisters': 267, 'no-quarter-given': 268, 'major': 269, 'toast': 270, 'then': 271, 'at': 272, 'huge': 273, 'went': 274, 'keep': 275, 'helped': 276, 'restaurants': 277, 'just': 278, 'days': 279, 'beef': 280, 'sub-par': 281, 'kerber': 282, 'full': 283, 'he': 284, 'knock': 285, 'missed': 286, 'was': 287, 'when': 288, 'more': 289, 'simona': 290, 'left': 291, 'player': 292, 'fans': 293, 'names': 294, 'quality': 295, 'third': 296, 'bit': 297, 'exhilarating': 298, 'week': 299, 'coworkers': 300, 'succeed': 301, 'odds': 302, 'caroline': 303, 'look': 304, 'tasted': 305, 'eggs': 306, 'hid': 307, 'big': 308, 'who': 309, '1.5': 310, 'extremely': 311, 'being': 312, 'no46': 313, 'minutes': 314, 'by': 315, 'quarter-final': 316, '33/1': 317, 'culminating': 318, 'losses': 319, 'wait': 320, 'served': 321, 'highly': 322, 'seal': 323, 'reached': 324, 'azarenka': 325, 'despite': 326, 'months': 327, 'case': 328, 'plus': 329, 'going': 330, 'to': 331, 'city': 332, 'resignation': 333, 'wins': 334, 'lead': 335, 'although': 336, 'said': 337, 'say': 338, 'bread': 339, 'half': 340, 'drinks': 341, 'liverpool': 342, 'unless': 343, 'potential': 344, 'haven': 345, 'north': 346, 'not': 347, 'seating': 348, 'proved': 349, 'outfit': 350, 'error': 351, 'defending': 352, 'seem': 353, 'last': 354, 'dinner': 355, 'finally': 356, 'quick': 357, 'ranks': 358, 'trophy': 359, 'four': 360, 'semis': 361, 'alisson': 362, 'has': 363, 'quickly': 364, 'can': 365, 'both': 366, 'tournaments': 367, 'dish': 368, 'pancakes': 369, 'did': 370, 'stunning': 371, 'thought': 372, 'signed': 373, 'in': 374, 'played': 375, 'flight': 376, 'summer': 377, 'action': 378, 'fair': 379, 'nn7': 380, 'world': 381, 'three': 382, 'champions': 383, 'area': 384, 'expected': 385, 'eat': 386, 'miami': 387, 'which': 388, 'ranked': 389, 'biscuits': 390, 'that': 391, 'ran': 392, 'especially': 393, 'before': 394, 'first': 395, 'open': 396, 'one': 397, 'never': 398, 'top': 399, 'event': 400, 'promote': 401, 'battle': 402, 'crumbs': 403, 'from': 404, 'reaching': 405, 'tie-break': 406, 'straight': 407, 'russia': 408, 'fc': 409, 'star': 410, 'rallies': 411, 'taste': 412, 'net': 413, 'quartet': 414, 'side': 415, 'struggled': 416, 'round': 417, 'games': 418, 'serena': 419, 'burgers': 420, 'double': 421, 'off': 422, 'course': 423, 'rakitic': 424, 'likely': 425, 'face': 426, 'his': 427, 'better': 428, 'does': 429, 'formaggi': 430, 'various': 431, 'reigning': 432, 'illness': 433, 'fairly': 434, 'really': 435, 'egg': 436, 'kitchen': 437, 'hodgson': 438, '1.5x': 439, 'over': 440, 'wimbledon': 441, 'moment': 442, 'many': 443, 'you': 444, 'chasing': 445, 'sauce': 446, 'iced': 447, 'hour': 448, 'within': 449, 'flavors': 450, 'omelettes': 451, 'sweet': 452, 'we': 453, '1': 454, 'winning': 455, 'of': 456, 'waffle': 457, 'saturday': 458, 'also': 459, 'stephens': 460, 'american': 461, 'doesnt': 462, 'portion': 463, 'plates': 464, 'excellent': 465, 'with': 466, 'team': 467, 'than': 468, 'seated': 469, 'hip': 470, 'looked': 471, 'turnaround': 472, 'montreal': 473, 'and': 474, 'were': 475, 'us': 476, 'first-round': 477, 'pessimistic': 478, 'fries': 479, 'quarter—except': 480, 'have': 481, 'however': 482, 'could': 483, 'no83': 484, 'signs': 485, 'person': 486, 'finished': 487, 'via': 488, 'here': 489, 'orders': 490, 'williams': 491, 'showed': 492, 'long': 493, 'there': 494, 'roma': 495, 'victory': 496, 'gave': 497, 'perisic': 498, '10-9': 499, 'gravy': 500, 'good': 501, 'salty': 502, 'small': 503, 'ostapenko': 504, 'restaurant': 505, 'sheets': 506, 'sliders': 507, 'year—and': 508, 'losing': 509, 'prepare': 510, 'place': 511, 'her': 512, 'great': 513, 'why': 514, 'sunnyvale': 515, 'card': 516, 'maybe': 517, 'chance': 518, 'run-in': 519, 'avoided': 520, 'pasta': 521, 'slower': 522, 'if': 523, 'york': 524, 'hubby': 525, 'semi-final': 526, 'oh-so-familiar': 527, 'hours': 528, 'people': 529, 'hungry': 530, 'like': 531, 'are': 532, 'failed': 533, 'since': 534, 'will': 535}\n"
     ]
    }
   ],
   "source": [
    "#word2idx = {}\n",
    "#for key in idx2word.keys():\n",
    "     #print(key, idx2word[key])\n",
    "     #word2idx[idx2word[key].strip()] = key\n",
    "#for key, value in idx2word.items():\n",
    "     #print(key, value)\n",
    "     #word2idx[value.strip()] = key\n",
    "#word2idx = { value.strip() : key for key, value in idx2word.items() }\n",
    "word2idx = { value.strip() : index for index, value in enumerate(results) }\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3bfbc-64c8-4d25-8960-243be6b0daef",
   "metadata": {},
   "source": [
    "***\n",
    "word2idx 딕셔너리를 활용해서 모든 지문을 구성하는 단어 목록을 수치로 변환한 파생 변수를 데이터프레임에 추가한다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec80076-1614-4c10-b774-9f153da871a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 145 218 374 515 138\n"
     ]
    }
   ],
   "source": [
    "# Dishplace is located in sunnyvale downtown -> 302 462 529 284 510 294\n",
    "print(word2idx['dishplace'], word2idx['is'], word2idx['located'], word2idx['in'], word2idx['sunnyvale'], word2idx['downtown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f867ac9-5258-4eef-83fa-ac0d5f926bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#지문을 구성하는 모든 단어를 수치로 변환하는 함수를 만든다.\n",
    "def encode_paragraph(paragragh):\n",
    "     #print(type(paragraph), paragragh)\n",
    "     words = paragragh.split()\n",
    "     #print(words)\n",
    "     encoded = []\n",
    "     for word in words:\n",
    "          encoded.append([word2idx[word.lower()]])\n",
    "     return encoded;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db447b4-f0af-409a-b567-d9f84f0063ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "      <th>enc_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[97], [145], [218], [374], [515], [138], [494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[207], [365], [134], [522], [161], [2], [528]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[83], [532], [273], [366], [11], [270], [474]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[453], [160], [466], [146], [330], [0], [46],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [390], [474], [500], [287], [78], [502],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [229], [479], [475], [112], [513], [209]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[103], [74], [287], [465], [77], [3], [0], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[61], [77], [262], [183], [189], [14], [57], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [341], [152], [155], [434], [364], [112]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[326], [0], [347], [175], [501], [73], [0], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [360], [432], [269], [383], [290], [30],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [176], [287], [12], [380], [489], [354],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[460], [245], [512], [194], [165], [404], [10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>When it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[288], [251], [152], [331], [21], [45], [374]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [467], [391], [258], [408], [89], [6], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [17], [350], [487], [374], [123], [511],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[342], [409], [535], [25], [331], [108], [139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[362], [373], [144], [342], [409], [404], [16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>But the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[174], [0], [222], [161], [391], [519], [331]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[271], [152], [0], [527], [53], [268], [402],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category  \\\n",
       "0   Dishplace is located in sunnyvale downtown the...     food   \n",
       "1   Service can be slower during busy hours but ou...     food   \n",
       "2   Portions are huge both french toast and their ...     food   \n",
       "3   We started with apps going the chicken and waf...     food   \n",
       "4   The biscuits and gravy was too salty two peopl...     food   \n",
       "5   The garlic fries were a great starter (and a h...     food   \n",
       "6   Our meal was excellent i had the pasta ai form...     food   \n",
       "7   What i enjoy most about palo alto is so many r...     food   \n",
       "8   The drinks came out fairly quickly a good two ...     food   \n",
       "9   Despite the not so good burger the service was...     food   \n",
       "10  The four reigning major champions simona halep...   sports   \n",
       "11  The briton was seeded nn7 here last year befor...   sports   \n",
       "12  Stephens surged her way back from injury in st...   sports   \n",
       "13  When it came to england chances in the world c...   sports   \n",
       "14  The team that eliminated russia – croatia – al...   sports   \n",
       "15  The perseyside outfit finished in fourth place...   sports   \n",
       "16  Liverpool fc will return to premier league act...   sports   \n",
       "17  Alisson signed for liverpool fc from as roma t...   sports   \n",
       "18  But the rankings during that run-in to new yor...   sports   \n",
       "19  Then came the oh-so-familiar djokovic-nadal no...   sports   \n",
       "\n",
       "                                        enc_paragraph  \n",
       "0   [[97], [145], [218], [374], [515], [138], [494...  \n",
       "1   [[207], [365], [134], [522], [161], [2], [528]...  \n",
       "2   [[83], [532], [273], [366], [11], [270], [474]...  \n",
       "3   [[453], [160], [466], [146], [330], [0], [46],...  \n",
       "4   [[0], [390], [474], [500], [287], [78], [502],...  \n",
       "5   [[0], [229], [479], [475], [112], [513], [209]...  \n",
       "6   [[103], [74], [287], [465], [77], [3], [0], [5...  \n",
       "7   [[61], [77], [262], [183], [189], [14], [57], ...  \n",
       "8   [[0], [341], [152], [155], [434], [364], [112]...  \n",
       "9   [[326], [0], [347], [175], [501], [73], [0], [...  \n",
       "10  [[0], [360], [432], [269], [383], [290], [30],...  \n",
       "11  [[0], [176], [287], [12], [380], [489], [354],...  \n",
       "12  [[460], [245], [512], [194], [165], [404], [10...  \n",
       "13  [[288], [251], [152], [331], [21], [45], [374]...  \n",
       "14  [[0], [467], [391], [258], [408], [89], [6], [...  \n",
       "15  [[0], [17], [350], [487], [374], [123], [511],...  \n",
       "16  [[342], [409], [535], [25], [331], [108], [139...  \n",
       "17  [[362], [373], [144], [342], [409], [404], [16...  \n",
       "18  [[174], [0], [222], [161], [391], [519], [331]...  \n",
       "19  [[271], [152], [0], [527], [53], [268], [402],...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지문을 수치로 변환된 결과를 데이터 프레임에 파생변수로 추가한다. -> [[302], [462], [529], [284], [510], [294]]\n",
    "df['enc_paragraph'] = df.paragraph.apply(encode_paragraph)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1943e-7327-4cdc-9469-ceeab6380dce",
   "metadata": {},
   "source": [
    "***\n",
    "분류 항목(food -> [1, 0], sports -> [0, 1])을 원-핫 인코딩으로 수치로 변환한 파생변수를 데이터프레임에 추가한다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7312c3a2-cc23-4954-843b-32c6bbf2b91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 분류 항목을 원-핫 인코딩 수치로 변환하는 함수를 만든다.\n",
    "def encode_category(category):\n",
    "     if category == 'food':\n",
    "          return [1, 0]\n",
    "     else :\n",
    "          return [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f092f1d-b5ea-4f3d-b9cc-5427d829436a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "      <th>enc_paragraph</th>\n",
       "      <th>enc_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[97], [145], [218], [374], [515], [138], [494...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[207], [365], [134], [522], [161], [2], [528]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[83], [532], [273], [366], [11], [270], [474]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[453], [160], [466], [146], [330], [0], [46],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [390], [474], [500], [287], [78], [502],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [229], [479], [475], [112], [513], [209]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[103], [74], [287], [465], [77], [3], [0], [5...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[61], [77], [262], [183], [189], [14], [57], ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [341], [152], [155], [434], [364], [112]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[326], [0], [347], [175], [501], [73], [0], [...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [360], [432], [269], [383], [290], [30],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [176], [287], [12], [380], [489], [354],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[460], [245], [512], [194], [165], [404], [10...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>When it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[288], [251], [152], [331], [21], [45], [374]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [467], [391], [258], [408], [89], [6], [...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [17], [350], [487], [374], [123], [511],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[342], [409], [535], [25], [331], [108], [139...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[362], [373], [144], [342], [409], [404], [16...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>But the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[174], [0], [222], [161], [391], [519], [331]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[271], [152], [0], [527], [53], [268], [402],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category  \\\n",
       "0   Dishplace is located in sunnyvale downtown the...     food   \n",
       "1   Service can be slower during busy hours but ou...     food   \n",
       "2   Portions are huge both french toast and their ...     food   \n",
       "3   We started with apps going the chicken and waf...     food   \n",
       "4   The biscuits and gravy was too salty two peopl...     food   \n",
       "5   The garlic fries were a great starter (and a h...     food   \n",
       "6   Our meal was excellent i had the pasta ai form...     food   \n",
       "7   What i enjoy most about palo alto is so many r...     food   \n",
       "8   The drinks came out fairly quickly a good two ...     food   \n",
       "9   Despite the not so good burger the service was...     food   \n",
       "10  The four reigning major champions simona halep...   sports   \n",
       "11  The briton was seeded nn7 here last year befor...   sports   \n",
       "12  Stephens surged her way back from injury in st...   sports   \n",
       "13  When it came to england chances in the world c...   sports   \n",
       "14  The team that eliminated russia – croatia – al...   sports   \n",
       "15  The perseyside outfit finished in fourth place...   sports   \n",
       "16  Liverpool fc will return to premier league act...   sports   \n",
       "17  Alisson signed for liverpool fc from as roma t...   sports   \n",
       "18  But the rankings during that run-in to new yor...   sports   \n",
       "19  Then came the oh-so-familiar djokovic-nadal no...   sports   \n",
       "\n",
       "                                        enc_paragraph enc_category  \n",
       "0   [[97], [145], [218], [374], [515], [138], [494...       [1, 0]  \n",
       "1   [[207], [365], [134], [522], [161], [2], [528]...       [1, 0]  \n",
       "2   [[83], [532], [273], [366], [11], [270], [474]...       [1, 0]  \n",
       "3   [[453], [160], [466], [146], [330], [0], [46],...       [1, 0]  \n",
       "4   [[0], [390], [474], [500], [287], [78], [502],...       [1, 0]  \n",
       "5   [[0], [229], [479], [475], [112], [513], [209]...       [1, 0]  \n",
       "6   [[103], [74], [287], [465], [77], [3], [0], [5...       [1, 0]  \n",
       "7   [[61], [77], [262], [183], [189], [14], [57], ...       [1, 0]  \n",
       "8   [[0], [341], [152], [155], [434], [364], [112]...       [1, 0]  \n",
       "9   [[326], [0], [347], [175], [501], [73], [0], [...       [1, 0]  \n",
       "10  [[0], [360], [432], [269], [383], [290], [30],...       [0, 1]  \n",
       "11  [[0], [176], [287], [12], [380], [489], [354],...       [0, 1]  \n",
       "12  [[460], [245], [512], [194], [165], [404], [10...       [0, 1]  \n",
       "13  [[288], [251], [152], [331], [21], [45], [374]...       [0, 1]  \n",
       "14  [[0], [467], [391], [258], [408], [89], [6], [...       [0, 1]  \n",
       "15  [[0], [17], [350], [487], [374], [123], [511],...       [0, 1]  \n",
       "16  [[342], [409], [535], [25], [331], [108], [139...       [0, 1]  \n",
       "17  [[362], [373], [144], [342], [409], [404], [16...       [0, 1]  \n",
       "18  [[174], [0], [222], [161], [391], [519], [331]...       [0, 1]  \n",
       "19  [[271], [152], [0], [527], [53], [268], [402],...       [0, 1]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류 항목이 수치로 변환된 결과를 데이터프레임에 추가한다.\n",
    "df['enc_category'] = df.category.apply(encode_category)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830b45c-b1e7-4b6b-909c-718214a9e89f",
   "metadata": {},
   "source": [
    "***\n",
    "각 입력값의 실제 길이(단어 갯수)를 알 수 있도록 각 지문별 단어의 갯수를 파생변수로 데이터프레임에 추가한다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa3ceaa-eae0-4895-928a-e7b2fd1a6a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 지문을 구성하는 단어의 갯수를 세는 함수를 만든다.\n",
    "def word_cnt(paragraph):\n",
    "     #print(paragraph.split())\n",
    "     #print('////')\n",
    "     return len(paragraph.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21baad54-562d-47f4-95a4-b0b1ed1f33b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "      <th>enc_paragraph</th>\n",
       "      <th>enc_category</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[97], [145], [218], [374], [515], [138], [494...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[207], [365], [134], [522], [161], [2], [528]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[83], [532], [273], [366], [11], [270], [474]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[453], [160], [466], [146], [330], [0], [46],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [390], [474], [500], [287], [78], [502],...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [229], [479], [475], [112], [513], [209]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[103], [74], [287], [465], [77], [3], [0], [5...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[61], [77], [262], [183], [189], [14], [57], ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[0], [341], [152], [155], [434], [364], [112]...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "      <td>[[326], [0], [347], [175], [501], [73], [0], [...</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [360], [432], [269], [383], [290], [30],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [176], [287], [12], [380], [489], [354],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[460], [245], [512], [194], [165], [404], [10...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>When it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[288], [251], [152], [331], [21], [45], [374]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [467], [391], [258], [408], [89], [6], [...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[0], [17], [350], [487], [374], [123], [511],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[342], [409], [535], [25], [331], [108], [139...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[362], [373], [144], [342], [409], [404], [16...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>But the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[174], [0], [222], [161], [391], [519], [331]...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "      <td>[[271], [152], [0], [527], [53], [268], [402],...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category  \\\n",
       "0   Dishplace is located in sunnyvale downtown the...     food   \n",
       "1   Service can be slower during busy hours but ou...     food   \n",
       "2   Portions are huge both french toast and their ...     food   \n",
       "3   We started with apps going the chicken and waf...     food   \n",
       "4   The biscuits and gravy was too salty two peopl...     food   \n",
       "5   The garlic fries were a great starter (and a h...     food   \n",
       "6   Our meal was excellent i had the pasta ai form...     food   \n",
       "7   What i enjoy most about palo alto is so many r...     food   \n",
       "8   The drinks came out fairly quickly a good two ...     food   \n",
       "9   Despite the not so good burger the service was...     food   \n",
       "10  The four reigning major champions simona halep...   sports   \n",
       "11  The briton was seeded nn7 here last year befor...   sports   \n",
       "12  Stephens surged her way back from injury in st...   sports   \n",
       "13  When it came to england chances in the world c...   sports   \n",
       "14  The team that eliminated russia – croatia – al...   sports   \n",
       "15  The perseyside outfit finished in fourth place...   sports   \n",
       "16  Liverpool fc will return to premier league act...   sports   \n",
       "17  Alisson signed for liverpool fc from as roma t...   sports   \n",
       "18  But the rankings during that run-in to new yor...   sports   \n",
       "19  Then came the oh-so-familiar djokovic-nadal no...   sports   \n",
       "\n",
       "                                        enc_paragraph enc_category  seq_length  \n",
       "0   [[97], [145], [218], [374], [515], [138], [494...       [1, 0]          53  \n",
       "1   [[207], [365], [134], [522], [161], [2], [528]...       [1, 0]          19  \n",
       "2   [[83], [532], [273], [366], [11], [270], [474]...       [1, 0]          42  \n",
       "3   [[453], [160], [466], [146], [330], [0], [46],...       [1, 0]          43  \n",
       "4   [[0], [390], [474], [500], [287], [78], [502],...       [1, 0]          82  \n",
       "5   [[0], [229], [479], [475], [112], [513], [209]...       [1, 0]          24  \n",
       "6   [[103], [74], [287], [465], [77], [3], [0], [5...       [1, 0]          50  \n",
       "7   [[61], [77], [262], [183], [189], [14], [57], ...       [1, 0]          43  \n",
       "8   [[0], [341], [152], [155], [434], [364], [112]...       [1, 0]          49  \n",
       "9   [[326], [0], [347], [175], [501], [73], [0], [...       [1, 0]          82  \n",
       "10  [[0], [360], [432], [269], [383], [290], [30],...       [0, 1]          65  \n",
       "11  [[0], [176], [287], [12], [380], [489], [354],...       [0, 1]          88  \n",
       "12  [[460], [245], [512], [194], [165], [404], [10...       [0, 1]          91  \n",
       "13  [[288], [251], [152], [331], [21], [45], [374]...       [0, 1]          71  \n",
       "14  [[0], [467], [391], [258], [408], [89], [6], [...       [0, 1]          70  \n",
       "15  [[0], [17], [350], [487], [374], [123], [511],...       [0, 1]          30  \n",
       "16  [[342], [409], [535], [25], [331], [108], [139...       [0, 1]          35  \n",
       "17  [[362], [373], [144], [342], [409], [404], [16...       [0, 1]          30  \n",
       "18  [[174], [0], [222], [161], [391], [519], [331]...       [0, 1]          63  \n",
       "19  [[271], [152], [0], [527], [53], [268], [402],...       [0, 1]          65  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지문을 구성하는 단어의 갯수를 데이터프레임에 파생변수로 추가한다.\n",
    "df['seq_length'] = df.paragraph.apply(word_cnt)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff2ef0-5759-4362-9d6d-33a27a736404",
   "metadata": {},
   "source": [
    "***\n",
    "RNN은 항상 같은 길이의 시퀀스를 받아야 한다.; 입력되는 값의 길이가 모두 같아야함  \n",
    "길이가 작은 입력 시퀀스는 패딩을 추가적으로 넣어서 모든 시퀀스 길이를 동일하게 설정한다.  \n",
    "패딩이 RNN 계산에 영향을 끼치지 않도록 패딩 이전의 입력 시퀀스의 실제 길이를 파라미터로 받아 상태값(출력값) 계산시 패딩을 제외한다.  \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34805e55-8621-45ab-b0ae-923f1725cbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "# 최고로 긴 지문의 단어의 수를 구한 후 모든 지문에 패딩을 집어넣어 최고로 긴 지문과 동일한 길이를 갖도록 만들기 위해 \n",
    "# 최고로 긴 지문의 단어의 수를 계산한다.\n",
    "#max_word_cnt = 0\n",
    "#for row in df.paragraph:\n",
    "     #print(type(row), row)\n",
    "     #print('/////')\n",
    "     #if len(row.split()) > max_word_cnt :\n",
    "          #max_word_cnt = len(row.split())\n",
    "     #--- if len\n",
    "#for seq in df.seq_length:\n",
    "#     if seq > max_word_cnt:\n",
    "#          max_word_cnt = seq\n",
    "\n",
    "max_word_cnt = df.seq_length.max()\n",
    "print(max_word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ea97aa-2af6-41ff-971e-ab6f7cdc7b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모든 지문에 패딩([-1])을 집어넣어 최고로 긴 문장과 동일한 길이를 갖게하는 함수를 만든다.\n",
    "def sequence_padding (enc_paragraph):\n",
    "     #print(type(enc_paragraph), enc_paragraph)\n",
    "     #print('/////')\n",
    "     #print(len(enc_paragraph))\n",
    "     for i in range(len(enc_paragraph), max_word_cnt):\n",
    "          enc_paragraph.append([-1])\n",
    "     \n",
    "     #for _ in range(max_word_cnt - len(enc_paragraph)):\n",
    "     #     enc_paragraph.append([-1])\n",
    "     #---- for _\n",
    "     return enc_paragraph\n",
    "     print(enc_paragraph)\n",
    "     #print(len(enc_paragraph))\n",
    "     #print('//////')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba0910ee-a1a4-4543-b6aa-09f3e0e690b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "[[207], [365], [134], [522], [161], [2], [528], [174], [103], [91], [287], [51], [474], [219], [497], [234], [513], [182], [141], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]]\n",
      "91\n",
      "[[460], [245], [512], [194], [165], [404], [100], [374], [371], [212], [331], [70], [512], [395], [269], [489], [354], [508], [389], [278], [484], [96], [363], [534], [349], [61], [112], [308], [179], [292], [96], [145], [455], [0], [387], [250], [488], [360], [79], [269], [383], [271], [405], [0], [166], [272], [0], [11], [396], [165], [169], [346], [461], [204], [50], [96], [392], [331], [0], [166], [374], [473], [181], [278], [132], [155], [315], [30], [96], [363], [459], [520], [443], [456], [0], [308], [294], [374], [512], [480], [144], [266], [516], [325], [163], [112], [252], [374], [0], [296], [417]]\n"
     ]
    }
   ],
   "source": [
    "# 모든 지문에 패딩이 적용된 결과를 enc_paragraph 컬럼에 적용한다.\n",
    "#df['encode_paragraph'] = df.enc_paragraph.apply(sequence_padding)\n",
    "df.enc_paragraph.apply(sequence_padding)\n",
    "print(len(df.enc_paragraph[1]))\n",
    "print(df.enc_paragraph[1])\n",
    "print(len(df.enc_paragraph[12]))\n",
    "print(df.enc_paragraph[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6cec2-764b-4bb2-85bb-aaff2bf7e1a8",
   "metadata": {},
   "source": [
    "***\n",
    "모델에서 사용할 입력값, 실제값(출력값), 지문의 실제 길이 파라미터를 만든다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa7b9345-0fa0-4495-bbee-8a623d3f8ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 91, 1) (20, 2)\n"
     ]
    }
   ],
   "source": [
    "# numpy의 array() 메소드는 인수로 파이썬의 리스트나 튜플 데이터를 받아서 numpy 배열로 변환하는 기능을 하기때문에\n",
    "# 데이터프레임의 시리즈를 바로 넣어주면 안되고, tolist() 메소드를 실행해서 파이썬의 리스트 형태로 변환한 후 넣어준다.\n",
    "#print(type(df.enc_paragraph)) #class 'Series'\n",
    "#print(type(df.enc_paragraph.to_list())) #class 'list'\n",
    "enc_paragraph = np.array(df.enc_paragraph.tolist()) # 입력값\n",
    "#print(enc_paragraph) # np.array\n",
    "#print(enc_paragraph)\n",
    "enc_category = np.array(df.enc_category.tolist()) # 실제값;레이블\n",
    "#print(enc_category)\n",
    "seq_length = np.array(df.seq_length.tolist()) # 지문의 실제 길이\n",
    "#print(seq_length)\n",
    "\n",
    "\n",
    "x_train = enc_paragraph\n",
    "y_train = enc_category\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f12a1d-eb0e-4780-be6d-de3a8019375a",
   "metadata": {},
   "source": [
    "***\n",
    "문맥 벡터(Contextualized Vector) 생성 단계\n",
    "***\n",
    "인덱스를 임베딩으로 변환한다.  \n",
    "임베딩은 학습과정을 통해 단어 유사도를 포함하게 되고 문맥 벡터를 생성하는데 도움을 준다.  \n",
    "인간의 언어(자연어)는 수치화돼 있지 않은 데이터이기때문에 머신러닝, 딥러닝 기법을 바로 사용할 수 없다.  \n",
    "그래서 자연어 처리에서 특징을 추출해서 수치화를 해줘야하는데 이때 사용하는 것이 '언어의 벡터화'이다. 이런 벡터화 과정을 word embedding이라고 한다.  \n",
    "LSTM에서 임베딩된 시퀀스를 입력해서 최종 상태값을 출력한다. -> 최종 상태값이 문맥 벡터이다.  \n",
    "\n",
    "***\n",
    "주제(food, sports) 분류 단계\n",
    "***\n",
    "문맥 벡터를 덴즈 레이어에 입력하고 출력값을 노드가 2개인 덴즈 레이어에 입력한 후 노드가 2개인 덴즈 레이어의 출력값을 소프트 맥에 입력시켜  \n",
    "food, sports에 대한 예측값을 계산한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd1ce0-7a43-4114-b652-d1a6d6fc9132",
   "metadata": {},
   "source": [
    "***\n",
    "지문을 읽고 주제를 분류하는 LSTM 모델을 만든다.\n",
    "***\n",
    "\n",
    "<img src=\"LSTM (1).png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aae12b-1302-461e-afb1-025f7827222c",
   "metadata": {},
   "source": [
    "***\n",
    "LSTM 모델을 구현한다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d5269c-4582-4612-8ae0-e74e8abdee21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tj\\AppData\\Local\\Temp\\ipykernel_13956\\339466534.py:16: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\tj\\.conda\\envs\\py3.7\\lib\\site-packages\\keras\\layers\\rnn\\legacy_cells.py:1048: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(35)\n",
    "\n",
    "#입력값(enc_paragraph)과 실제값(enc_category)을 저장할 placeholder를 만든다.\n",
    "# 입력값은 문장을 구성하는 단어들의 인덱스이며, 그 길이는 지문을 구성하는 단어의 최대 갯수(max_word_cnt)이다.\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, max_word_cnt, 1)) #입력값을 기억할 placeholder (20, 91, 1)\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 2)) #실제값을 기억할 placeholder (20, 2) 20은 자료의 갯수; 후에 증가할지 감소할지 모르므로 placeholder -> none\n",
    "\n",
    "#임베딩 레이어는 입력값(단어들의 인덱스)을 입력받아 5차원의 벡터 임베딩을 출력한다.\n",
    "#layers.dense() 메소드는 densely-connect layer 즉, '완전 연결 계층'을 만들어 준다.\n",
    "embedding = tf.layers.dense(x, 5)\n",
    "\n",
    "#LSTM 셀은 64차원의 벡터의 생성값을 출력한다.\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64)\n",
    "\n",
    "# LSTM 셀의 출력값과 상태값을 저장한다.\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, dtype=tf.float32, inputs=embedding, sequence_length=seq_length)\n",
    "\n",
    "# 주제 분류는 두개의 덴즈 레이어를 사용한다.\n",
    "# 첫번째 덴즈 레이어는 32개의 노드를 가지고 있고, 두번쨰 덴즈 레이어는 2개의 노드를 가지고 있으며\n",
    "# 이 2개의 노드가 소프트 맥스의 입력으로 들어간다.\n",
    "dense_layer = tf.layers.dense(state.h, 32) # c -> 메모리셀 값, h -> 실제 출력값\n",
    "\n",
    "# logit은 food, sports를 원-핫 인코딩으로 구분하기위해 2차원 벡터로 구성한다. \n",
    "logit = tf.layers.dense(dense_layer, 2) # 최종 예측값\n",
    "\n",
    "# 손실 함수 -> 크로스 엔트로피\n",
    "loss = tf.reduce_mean( \n",
    "     tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y)\n",
    ")\n",
    "\n",
    "# Adam 옵티마이저\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3e043-34f2-4794-938d-d9adc5bdd298",
   "metadata": {},
   "source": [
    "***\n",
    "모델 요약\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c77e36c-5c57-49e5-a1fc-be8951e8f106",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 91, 1), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 91, 5), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_4:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#문맥 벡터 생성\n",
    "#입력값은 단어들의 인덱스이며 그 길이는 항상 91이다.\n",
    "print(x) #Tensor(\"Placeholder:0\", shape=(?, 91, 1), dtype=float32)\n",
    "\n",
    "#임베딩 레이어는 인덱스를 받아 5차원 벡터의 임베딩을 출력한다.\n",
    "print(embedding) #Tensor(\"dense/BiasAdd:0\", shape=(?, 91, 5), dtype=float32)\n",
    "\n",
    "# LSTM 셀은 64차원의 상태값을 출력한다.\n",
    "print(state.h) #Tensor(\"rnn/while/Exit_4:0\", shape=(?, 64), dtype=float32)\n",
    "\n",
    "\n",
    "#문맥 벡터를 사용해서 지문의 주제 분류하기, 주제 분류에는 2개의 덴즈 레이어를 사용했다.\n",
    "#첫번째 덴즈 레이어는 32개의 노드를 가진다. \n",
    "print(dense_layer) #Tensor(\"dense_1/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
    "\n",
    "#두번째 덴즈 레이어는 2개의 노드를 가지고 있으며, 이 2개의 노드는 소프트 맥스의 입력으로 들어간다.\n",
    "print(logit) #Tensor(\"dense_2/BiasAdd:0\", shape=(?, 2), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09aa0e6-3a22-4912-a48e-d0b6fe55240b",
   "metadata": {},
   "source": [
    "***\n",
    "학습시킨다.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "187f8b32-ee57-4322-b099-09cd4bac1b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch ->   0, loss(오차) -> 0.7384, accuracy(정확도) -> 0.5000\n",
      "epoch ->  20, loss(오차) -> 0.5219, accuracy(정확도) -> 0.8500\n",
      "epoch ->  40, loss(오차) -> 0.3175, accuracy(정확도) -> 0.9000\n",
      "epoch ->  60, loss(오차) -> 0.0900, accuracy(정확도) -> 1.0000\n",
      "epoch ->  80, loss(오차) -> 0.0183, accuracy(정확도) -> 1.0000\n",
      "epoch -> 100, loss(오차) -> 0.0057, accuracy(정확도) -> 1.0000\n",
      "epoch -> 120, loss(오차) -> 0.0026, accuracy(정확도) -> 1.0000\n",
      "epoch -> 140, loss(오차) -> 0.0015, accuracy(정확도) -> 1.0000\n",
      "epoch -> 160, loss(오차) -> 0.0009, accuracy(정확도) -> 1.0000\n",
      "epoch -> 180, loss(오차) -> 0.0009, accuracy(정확도) -> 1.0000\n",
      "epoch -> 200, loss(오차) -> 0.0004, accuracy(정확도) -> 1.0000\n",
      "epoch -> 220, loss(오차) -> 0.0003, accuracy(정확도) -> 1.0000\n",
      "epoch -> 240, loss(오차) -> 0.0002, accuracy(정확도) -> 1.0000\n",
      "epoch -> 260, loss(오차) -> 0.0002, accuracy(정확도) -> 1.0000\n",
      "epoch -> 280, loss(오차) -> 0.0001, accuracy(정확도) -> 1.0000\n",
      "epoch -> 300, loss(오차) -> 0.0001, accuracy(정확도) -> 1.0000\n",
      "epoch -> 320, loss(오차) -> 0.0001, accuracy(정확도) -> 1.0000\n",
      "epoch -> 340, loss(오차) -> 0.0001, accuracy(정확도) -> 1.0000\n",
      "epoch -> 360, loss(오차) -> 0.0001, accuracy(정확도) -> 1.0000\n",
      "epoch -> 380, loss(오차) -> 0.0000, accuracy(정확도) -> 1.0000\n",
      "epoch -> 400, loss(오차) -> 0.0000, accuracy(정확도) -> 1.0000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "     sess.run(tf.global_variables_initializer())\n",
    "     for epoch in range(401):\n",
    "          _, loss_ = sess.run([train, loss], feed_dict={x: x_train, y: y_train})\n",
    "          \n",
    "          \n",
    "          if epoch % 20 == 0:\n",
    "               predict = tf.nn.softmax(logits=logit) #최종 예측값\n",
    "               # 정확도를 계산하는 수식을 만든다.\n",
    "               correct_predict = tf.equal(tf.argmax(predict, axis=1), tf.argmax(y, axis=1))\n",
    "               accuracy = tf.reduce_mean(tf.cast(correct_predict, dtype=tf.float32))\n",
    "               current_accuracy = accuracy.eval(feed_dict={x: x_train, y: y_train}) # 정확도 계산\n",
    "               print('epoch -> {:3d}, loss(오차) -> {:6.4f}, accuracy(정확도) -> {:6.4f}'.format(epoch, loss_, current_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaaadf1-a1ba-4704-9961-0710b6266252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5ad80-924b-4304-b08f-b1db65fff738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b32c41-5c44-41c2-aef7-e42582963c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953f353-f7cc-48a9-89b9-dfbec2dbea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456ff09-c8cb-403e-a1e5-4330ab4073f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434c32e-5a75-4f09-a2c8-4337b67f300f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2e028-3ac2-4962-949e-cde6fdf4b891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4863b14-bc32-40cd-8281-e137ead03e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6b05b-45e4-4598-8807-247eac726726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd441a-45d5-444c-b5a0-4c459c591096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ce40c-4a59-4f2f-9337-1c7fefb680ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5237ea-e550-4850-986d-d56a6b0298ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4814bd-adb5-48ed-bac8-0f5cca294efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a49ee7-a4bc-4d31-914a-9248ffea5be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f88d8-04aa-4d3c-88c0-9378d0b2a2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2287a-93bd-4a23-965b-9a8527d4cbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a796653-031d-4046-adf6-bd36e1f70feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f8d4f-7469-4137-9b7a-9cfce2aa61ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6851e2-77ed-48e3-b553-247d68e7f321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeef310-a893-474a-b472-083d4271216d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98531d-72d6-4241-88c8-1e9c463eed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de8f98-c9bb-4a60-a33a-07bbf3675d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36749cb-da81-45e4-a646-31fda4b70e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe969b3-b697-4206-8a32-0924d2a87035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c242ba-a25e-4337-bfe5-bd6b952465b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849b58d-da98-4c17-8145-63a5a7c50cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a303b68-152a-49cc-899e-314b675617de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py3.7] *",
   "language": "python",
   "name": "conda-env-.conda-py3.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
